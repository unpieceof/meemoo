"""Pipeline workers: Analyst, Librarian, Recommender."""
from __future__ import annotations

import json
import re
from datetime import datetime, timezone

from . import claude_client, supabase_client, extractor
from .schemas import ANALYST_SCHEMA, RECOMMENDER_SCHEMA

PAGE_SIZE = 5


# Librarian Searchìš© function â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CATEGORY_ICON = {
    "ì¼": "ğŸ§‘â€ğŸ’»",
    "ë°°ì›€": "ğŸ“˜",
    "ì•„ì´ë””ì–´": "ğŸ’¡",
    "ì •ë³´": "ğŸ“°",
    "ê¸°ë¡": "ğŸ“",
    "ë¬¸í™”": "ğŸ¬",
    "ì†Œë¹„": "ğŸ½",
}

def _primary_tag(tags):
    if not tags:
        return ""
    t = str(tags[0]).strip()
    return t[:10]  # ê³¼ë„í•˜ê²Œ ê¸¸ë©´ ì»·(ì„ íƒ)

def _preview_from_bullets(bullets):
    if isinstance(bullets, list) and bullets:
        s = str(bullets[0]).strip()
        return (s[:70] + "â€¦") if len(s) > 70 else s
    return ""

def _fmt_date(created_at: str) -> str:
    # "2026-02-16 13:09:23.779322+00"
    try:
        dt = datetime.fromisoformat(created_at.replace(" ", "T"))
        return dt.strftime("%m.%d")
    except Exception:
        return ""

def _decorate(m: dict) -> dict:
    cat = (m.get("category") or "").strip()
    icon = CATEGORY_ICON.get(cat, "ğŸ·")
    tag = _primary_tag(m.get("tags"))
    title = (m.get("title") or "").strip()
    date = _fmt_date(m.get("created_at") or "")

    # í•µì‹¬: ì§ê´€ì  1ì¤„ íƒ€ì´í‹€
    left = []
    if cat:
        left.append(cat)
    if tag:
        left.append(tag)

    prefix = f"{icon} \\[{' Â· '.join(left)}\\]"
    m["display_title"] = f"{prefix} {title}".strip()

    # í•µì‹¬: ê²€ìƒ‰ìš© 1ì¤„ í”„ë¦¬ë·°
    m["display_preview"] = _preview_from_bullets(m.get("summary_bullets"))

    # ë³´ì¡°: ë‚ ì§œ
    if date:
        m["display_date"] = date

    # ëŒ€í‘œ íƒœê·¸ë„ ë”°ë¡œ (í•„í„°/ì¹© UIìš©)
    if tag:
        m["display_tag"] = tag

    return m


# â”€â”€ Analyst (ğŸ”) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def analyst_run(payload: str) -> dict:
    """Extract URL (with optional user context) -> call Claude -> return analysis JSON."""
    url_match = re.search(r"https?://\S+", payload)
    url = url_match.group(0) if url_match else ""
    # Detect bare domain (e.g. griddyicons.com) and prepend https://
    if not url:
        domain_match = re.search(r"\b([\w-]+\.(?:com|net|org|io|co|dev|ai|kr|me|app|xyz))\b", payload, re.I)
        if domain_match:
            url = f"https://{domain_match.group(1)}"
    user_context = payload.replace(url_match.group(0) if url_match else "", "").strip() if url else payload

    source_type, extracted = extractor.extract_text(url) if url else ("web", "")

    parts = []
    if user_context:
        parts.append(f"ì‚¬ìš©ì ë©”ëª¨: {user_context}")
    if extracted:
        parts.append(f"í˜ì´ì§€ ë‚´ìš©: {extracted}")
    text = "\n\n".join(parts) or payload

    result = claude_client.ask_json(
        system=(
            "You are a concise analyst. Given text (possibly with user notes and webpage content), "
            "produce a memo with: title (Korean), 3 bullet summary (Korean), category, tags."
            "[ì¶œë ¥ ê·œì¹™]"
            "title: 10~25ì, í•µì‹¬ í¬í•¨"
            "summary_bullets: 1~3ê°œ, ê° 15~35ì, ì»¤ë®¤ë‹ˆí‹° ë§íˆ¬(ì˜ˆ: â€œ~í•˜ë©´ ë¨â€, â€œ~ì¸ ë“¯â€, â€œìš”ì•½í•˜ë©´â€), ì‰¬ìš´ ë§ + êµ°ë”ë”ê¸° ì œê±°"
            "category: [ì¼, ë°°ì›€, ì•„ì´ë””ì–´, ì •ë³´, ê¸°ë¡, ë¬¸í™”, ì†Œë¹„] ì¤‘ 1ê°œ"
            "tags: 2~5ê°œ ëª…ì‚¬í˜•(2~10ì), ê²€ìƒ‰ì´ ì‰½ë„ë¡ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ê³ , ì‹ë‹¹/ì¹´í˜ ê´€ë ¨ ë©”ëª¨ëŠ” ë¬´ì¡°ê±´ ë§›ì§‘ íƒœê·¸ë¥¼ ì¶”ê°€. ì¥ì†Œê°€ ìˆìœ¼ë©´ ì¥ì†Œ(2~5ì) í‚¤ì›Œë“œë¥¼ ë°˜ë“œì‹œ ì¶”ê°€."
        ),
        user=text,
        schema=ANALYST_SCHEMA,
    )
    result["source_url"] = url or ""
    result["source_type"] = source_type
    result["_raw_content"] = text  # for storage
    return result


# â”€â”€ Librarian (ğŸ“š) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def librarian_run(action_payload: str, analyst_result: dict | None = None) -> dict:
    """Handle save/list/search/delete."""
    action, _, payload = action_payload.partition(":")

    if action == "save" or analyst_result is not None:
        if analyst_result is None:
            return {"error": "No analyst result to save"}

        src_url = analyst_result["source_url"]

        # â”€â”€ Dedup check â”€â”€
        if src_url and not src_url.startswith("memo://"):
            existing = supabase_client.find_by_url(src_url)
            if existing:
                return {
                    "action": "duplicate",
                    "existing_id": existing["id"],
                    "existing_title": existing["title"],
                }

        if not src_url:
            src_url = f"memo://{datetime.now(timezone.utc).strftime('%Y%m%d%H%M%S%f')}"

        memo = {
            "title": analyst_result["title"],
            "summary_bullets": analyst_result["bullets"],
            "category": analyst_result["category"],
            "tags": analyst_result["tags"],
            "source_url": src_url,
            "source_type": analyst_result["source_type"],
            "raw_content": analyst_result.get("_raw_content", "")[:8000],
            "created_at": datetime.now(timezone.utc).isoformat(),
        }
        saved = supabase_client.upsert_memo(memo)
        return {"action": "saved", "memo": saved[0] if saved else memo}

    if action == "list":
        page = 0
        if payload.strip().isdigit():
            page = int(payload.strip())
        offset = page * PAGE_SIZE
        memos = supabase_client.list_memos(limit=PAGE_SIZE, offset=offset)
        total = supabase_client.count_memos()
        return {"action": "list", "memos": memos, "page": page, "total": total}

    if action == "search":
        # Parse page from payload: "query:page" or just "query"
        page = 0
        query = payload
        if ":" in payload:
            parts = payload.rsplit(":", 1)
            if parts[-1].strip().isdigit():
                query = parts[0]
                page = int(parts[-1].strip())

        offset = page * PAGE_SIZE
        memos, total = supabase_client.search_memos_text(query, limit=PAGE_SIZE, offset=offset)
        memos = [_decorate(m) for m in memos]

        return {"action": "search", "query": query, "memos": memos, "page": page, "total": total}

    if action == "category":
        if not payload:
            # No category specified -> show counts per category
            counts = supabase_client.get_category_counts()
            return {"action": "category_list", "counts": counts}
        memos = supabase_client.get_memos_by_category(payload)
        return {"action": "category", "category": payload, "memos": memos}

    if action == "view":
        memo = supabase_client.get_memo_by_id(payload)
        return {"action": "view", "memo": memo}

    if action == "delete":
        ok = supabase_client.delete_memo(payload)
        return {"action": "delete", "memo_id": payload, "success": ok}

    return {"error": f"Unknown librarian action: {action}"}


# â”€â”€ Recommender (ğŸ’¡) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def recommender_run(payload: str, max_categories: int = 3, memos: list | None = None) -> dict:
    """Recommend memos grouped by category. Only when explicitly requested."""
    metas = memos if memos is not None else supabase_client.get_random_memos_by_category(per_category=1, max_categories=max_categories)
    if not metas:
        return {"categories": []}

    result = claude_client.ask_json(
        system=(
            "ë„ˆëŠ” 'ë©”ëª¨ ì¶”ì²œ íë ˆì´í„°'ì•¼.\n"
            "ì…ë ¥ì€ ë©”ëª¨ ëª©ë¡(JSON ë°°ì—´)ì´ê³ , ê° ë©”ëª¨ëŠ” id/title/summary_bullets/category/tagsë¥¼ ê°€ì§„ë‹¤.\n\n"
            "ëª©í‘œ: ê° ë©”ëª¨ì— ëŒ€í•´ ì¹´í…Œê³ ë¦¬ë³„ ì¶”ì²œ ê²°ê³¼(JSON)ë¥¼ ë°˜í™˜.\n\n"
            "ê·œì¹™:\n"
            "1) memo.categoryë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¬¶ì–´.\n"
            "   - categoryê°€ ë„ˆë¬´ ë„“ìœ¼ë©´ tagsë¥¼ ì°¸ê³ í•´ ë” ì§ê´€ì ì¸ ì´ë¦„ìœ¼ë¡œ ë°”ê¿”ë„ ë¨.\n"
            f"2) ì¹´í…Œê³ ë¦¬ëŠ” ìµœëŒ€ {max_categories}ê°œ.\n"
            "3) ê° ì¹´í…Œê³ ë¦¬ì—ëŠ” emoji 1ê°œ, one_liner(ì»¤ë®¤ë‹ˆí‹° ë§íˆ¬ë¡œ ìê·¹ì ì¸ í•œ ì¤„ ì†Œê°œ) 1ê°œ.\n"
            "4) ê° ì¹´í…Œê³ ë¦¬ itemsëŠ” ì…ë ¥ëœ ë©”ëª¨ ì „ë¶€ í¬í•¨.\n"
            "5) ê° itemì—ëŠ”:\n"
            "   - memo_id: ì…ë ¥ì˜ id ê·¸ëŒ€ë¡œ\n"
            "   - title: ì…ë ¥ì˜ title ê·¸ëŒ€ë¡œ\n"
            "   - preview: summary_bullets ì¤‘ ê°€ì¥ 'ì•„ ì´ê±°!' ì‹¶ì€ 1ê°œ\n"
            "   - reason: ì§§ê³  ì§ê´€ì ìœ¼ë¡œ ì™œ ì§€ê¸ˆ ë´ì•¼ í•˜ëŠ”ì§€\n"
            "   - tags: ì…ë ¥ tags ì¤‘ í•µì‹¬ 2~4ê°œë§Œ\n"
            "6) ë§íˆ¬: ì¹œê·¼ + ì‚´ì§ ìê·¹(ì»¤ë®¤ë‹ˆí‹° í†¤). ì •ë³´ëŠ” ì •í™•í•˜ê²Œ.\n"
        ),
        user=json.dumps(metas, ensure_ascii=False),
        schema=RECOMMENDER_SCHEMA,
        max_tokens=1024,
    )
    return result

